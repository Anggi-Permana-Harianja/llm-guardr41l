import * as vscode from 'vscode';
import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import { RulesConfig, generatePromptDirectives } from './rules';

export type LLMProvider = 'openai' | 'anthropic';

export interface LLMConfig {
  provider: LLMProvider;
  apiKey: string;
  model: string;
}

export interface GenerateCodeRequest {
  prompt: string;
  context: string;
  rules: RulesConfig;
  fileName?: string;
  language?: string;
}

export interface GenerateCodeResponse {
  success: boolean;
  generatedCode?: string;
  error?: string;
  tokensUsed?: number;
  model?: string;
}

function getConfig(): LLMConfig {
  const config = vscode.workspace.getConfiguration('llm-guardrail');
  const provider = config.get<LLMProvider>('provider', 'openai');
  const model = config.get<string>('model', provider === 'openai' ? 'gpt-4o' : 'claude-3-5-sonnet-20241022');

  let apiKey: string;
  if (provider === 'openai') {
    apiKey = config.get<string>('openaiApiKey', '') || process.env.OPENAI_API_KEY || '';
  } else {
    apiKey = config.get<string>('anthropicApiKey', '') || process.env.ANTHROPIC_API_KEY || '';
  }

  return { provider, apiKey, model };
}

function buildSystemPrompt(rules: RulesConfig, language?: string): string {
  const directives = generatePromptDirectives(rules);

  let systemPrompt = `You are a code generation assistant. Your task is to generate or modify code based on user requests while strictly following the provided rules.

${directives}

CRITICAL INSTRUCTIONS:
1. Only output the code itself - no explanations, no markdown code blocks, no comments about the code.
2. Maintain the exact indentation and formatting style of the provided context.
3. Do NOT add any code beyond what is explicitly requested.
4. Do NOT refactor, optimize, or "improve" existing code unless specifically asked.
5. Do NOT add error handling, validation, or defensive code unless specifically asked.
6. Do NOT add comments, documentation, or type annotations unless specifically asked.
7. Do NOT rename variables, functions, or classes unless specifically asked.
8. Preserve all existing code that is not directly related to the requested change.
`;

  if (language) {
    systemPrompt += `\nThe code is written in ${language}. Follow the conventions and best practices for this language.`;
  }

  return systemPrompt;
}

function buildUserPrompt(request: GenerateCodeRequest): string {
  let userPrompt = '';

  if (request.context && request.context.trim()) {
    userPrompt += `Here is the existing code context:\n\n${request.context}\n\n`;
  }

  userPrompt += `User request: ${request.prompt}\n\n`;
  userPrompt += `Respond with ONLY the complete modified code. No explanations, no markdown formatting, just the code.`;

  return userPrompt;
}

async function generateWithOpenAI(
  config: LLMConfig,
  systemPrompt: string,
  userPrompt: string
): Promise<GenerateCodeResponse> {
  const client = new OpenAI({ apiKey: config.apiKey });

  try {
    const response = await client.chat.completions.create({
      model: config.model,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature: 0.2,
      max_tokens: 4096
    });

    const generatedCode = response.choices[0]?.message?.content?.trim();

    if (!generatedCode) {
      return {
        success: false,
        error: 'No code generated by the model'
      };
    }

    return {
      success: true,
      generatedCode: cleanCodeOutput(generatedCode),
      tokensUsed: response.usage?.total_tokens,
      model: config.model
    };
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : 'Unknown error';
    return {
      success: false,
      error: `OpenAI API error: ${errorMessage}`
    };
  }
}

async function generateWithAnthropic(
  config: LLMConfig,
  systemPrompt: string,
  userPrompt: string
): Promise<GenerateCodeResponse> {
  const client = new Anthropic({ apiKey: config.apiKey });

  try {
    const response = await client.messages.create({
      model: config.model,
      max_tokens: 4096,
      system: systemPrompt,
      messages: [
        { role: 'user', content: userPrompt }
      ]
    });

    const textBlock = response.content.find(block => block.type === 'text');
    const generatedCode = textBlock && textBlock.type === 'text' ? textBlock.text.trim() : undefined;

    if (!generatedCode) {
      return {
        success: false,
        error: 'No code generated by the model'
      };
    }

    return {
      success: true,
      generatedCode: cleanCodeOutput(generatedCode),
      tokensUsed: response.usage.input_tokens + response.usage.output_tokens,
      model: config.model
    };
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : 'Unknown error';
    return {
      success: false,
      error: `Anthropic API error: ${errorMessage}`
    };
  }
}

function cleanCodeOutput(code: string): string {
  // Remove markdown code blocks if present
  let cleaned = code;

  // Remove ```language and ``` wrappers
  const codeBlockMatch = cleaned.match(/^```[\w]*\n?([\s\S]*?)\n?```$/);
  if (codeBlockMatch) {
    cleaned = codeBlockMatch[1];
  }

  // Remove leading/trailing whitespace but preserve internal formatting
  cleaned = cleaned.trim();

  return cleaned;
}

export async function generateCode(request: GenerateCodeRequest): Promise<GenerateCodeResponse> {
  const config = getConfig();

  if (!config.apiKey) {
    return {
      success: false,
      error: `No API key configured for ${config.provider}. Please set it in VS Code settings (llm-guardrail.${config.provider}ApiKey) or as an environment variable.`
    };
  }

  const systemPrompt = buildSystemPrompt(request.rules, request.language);
  const userPrompt = buildUserPrompt(request);

  if (config.provider === 'openai') {
    return generateWithOpenAI(config, systemPrompt, userPrompt);
  } else {
    return generateWithAnthropic(config, systemPrompt, userPrompt);
  }
}

export function getCurrentProvider(): LLMProvider {
  return getConfig().provider;
}

export function getCurrentModel(): string {
  return getConfig().model;
}

export function isConfigured(): boolean {
  const config = getConfig();
  return Boolean(config.apiKey);
}
